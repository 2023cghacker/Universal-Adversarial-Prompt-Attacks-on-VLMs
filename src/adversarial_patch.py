import torch
import torch.optim as optim
import torch.nn.functional as F
from PIL import Image
from torchvision import transforms
from datetime import datetime
import matplotlib.pyplot as plt
from adversarial_base import CLIPAdversarialBase
import os
import sys
from tqdm import tqdm  # å¯¼å…¥tqdm


class CLIPAdversarialPatch(CLIPAdversarialBase):
    def __init__(self, model_path, device=None, num_steps=300, lr=1e-3):
        """
        å±€éƒ¨å¯¹æŠ—è¡¥ä¸è®­ç»ƒç±»ï¼šä»…å®ç°train_patchæ–¹æ³•
        ç»§æ‰¿è‡ªCLIPAdversarialBaseï¼Œå¤ç”¨æ‰€æœ‰å…¬å…±åŠŸèƒ½
        """
        super().__init__(model_path, device, num_steps, lr)

    def train_patch(
        self,
        background_image_paths,
        target_text=None,
        target_img=None,
        patch_size=80,
        positions=[[30, 30]],
        background_weight=0.1,
        initial_patch_path=None,
        save_names=None,
    ):
        """
        æ ¸å¿ƒåŠŸèƒ½ï¼šè®­ç»ƒå±€éƒ¨å¯¹æŠ—è¡¥ä¸ï¼ˆåœ¨å¤šå¼ èƒŒæ™¯å›¾ä¸Šå åŠ è¡¥ä¸ï¼Œä½¿CLIPè¯¯åˆ†ç±»ï¼‰
        :param background_image_paths: èƒŒæ™¯å›¾åƒè·¯å¾„åˆ—è¡¨
        :param target_text: ç›®æ ‡æ–‡æœ¬ï¼ˆä¸target_imgäºŒé€‰ä¸€ï¼‰
        :param target_img: ç›®æ ‡å›¾åƒï¼ˆä¸target_textäºŒé€‰ä¸€ï¼‰
        :param patch_size: è¡¥ä¸å°ºå¯¸ï¼ˆæ­£æ–¹å½¢ï¼‰
        :param positions: è¡¥ä¸åœ¨èƒŒæ™¯å›¾ä¸Šçš„ä½ç½®åˆ—è¡¨ï¼ˆä¸èƒŒæ™¯å›¾æ•°é‡ä¸€è‡´ï¼‰
        :param background_weight: èƒŒæ™¯æŸå¤±æƒé‡ï¼ˆæ§åˆ¶è¡¥ä¸ä¸åŸå›¾ç›¸ä¼¼åº¦ï¼‰
        :param initial_patch_path: åˆå§‹è¡¥ä¸å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        :param save_names: ä¿å­˜åç§°åˆ—è¡¨ï¼ˆä¸èƒŒæ™¯å›¾æ•°é‡ä¸€è‡´ï¼‰
        :return: ä¿å­˜çš„å›¾åƒè·¯å¾„åˆ—è¡¨
        """
        # è¾“å…¥åˆæ³•æ€§æ ¡éªŒ
        if not ((target_text is None) ^ (target_img is None)):
            raise ValueError("target_textä¸target_imgå¿…é¡»äºŒé€‰ä¸€")
        if len(background_image_paths) != len(positions):
            raise ValueError("èƒŒæ™¯å›¾è·¯å¾„åˆ—è¡¨ä¸ä½ç½®åˆ—è¡¨é•¿åº¦å¿…é¡»ä¸€è‡´")
        if save_names is None or len(save_names) != len(background_image_paths):
            raise ValueError("ä¿å­˜åç§°åˆ—è¡¨å¿…é¡»æä¾›ä¸”ä¸èƒŒæ™¯å›¾æ•°é‡ä¸€è‡´")

        """1.è·å–ç›®æ ‡åµŒå…¥ï¼ˆæ–‡æœ¬æˆ–å›¾åƒï¼‰""" 
        if target_text is not None:
            target_emb = self.get_target_text_embedding(target_text)
        else:
            target_emb = self.get_target_image_embedding(target_img)


        """2.åŠ è½½æ‰€æœ‰èƒŒæ™¯å›¾åƒ """ 
        img_tensors = []
        for img_path in background_image_paths:
            img_tensor, _ = self.load_and_preprocess_image(img_path)
            img_tensors.append(img_tensor)

        """3.åˆå§‹åŒ–å¯¹æŠ—patch """
        # æ ¡éªŒæ‰€æœ‰è¡¥ä¸ä½ç½®æ˜¯å¦è¶…å‡ºå›¾åƒè¾¹ç•Œ
        for i, (img_tensor, pos) in enumerate(zip(img_tensors, positions)):
            x, y = pos
            _, _, h, w = img_tensor.shape
            if x < 0 or x + patch_size > w or y < 0 or y + patch_size > h:
                raise ValueError(f"ç¬¬{i}ä¸ªè¡¥ä¸ä½ç½®è¶…å‡ºå›¾åƒè¾¹ç•Œï¼ˆå›¾åƒå°ºå¯¸ï¼š{w}x{h}ï¼‰")
            
        # åˆ›å»ºpatch
        if initial_patch_path is not None: # ä»æŒ‡å®šè·¯å¾„åŠ è½½å¹¶Resizeä¸ºè¡¥ä¸å°ºå¯¸
            trans = transforms.Compose(
                [
                    transforms.Resize((patch_size, patch_size)),
                    transforms.ToTensor(),
                    transforms.Lambda(lambda x: x.unsqueeze(0)),  # å¢åŠ æ‰¹æ¬¡ç»´åº¦
                ]
            )
            patch_img = Image.open(initial_patch_path).convert("RGB")
            patch = trans(patch_img).to(self.device)
        else: # ä»èƒŒæ™¯å›¾çš„æŒ‡å®šä½ç½®æˆªå–å¹¶æ·»åŠ å™ªå£°
            x_init, y_init = positions[0]
            background_patch = img_tensors[0][
                :, :, y_init : y_init + patch_size, x_init : x_init + patch_size
            ].clone()
            patch = background_patch * 0.8 + torch.randn_like(background_patch) * 0.2

        # ä¿å­˜åŸå§‹è¡¥ä¸ï¼ˆç”¨äºè®¡ç®—èƒŒæ™¯æŸå¤±ï¼‰
        original_patches = []
        for img_tensor, pos in zip(img_tensors, positions):
            x, y = pos
            original_patch = img_tensor[
                :, :, y : y + patch_size, x : x + patch_size
            ].detach()
            original_patches.append(original_patch)

        """4.è®­ç»ƒpatch """
        # ä¼˜åŒ–å™¨é…ç½®
        patch.requires_grad_(True)
        optimizer = optim.Adam([patch], lr=self.lr)

        # è®­ç»ƒè®°å½•åˆå§‹åŒ–
        start_time = datetime.now()
        loss_history = []
        similarity_history = []

        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        print("\n\nStart patch training...")
        progress_bar = tqdm(range(1, self.num_steps + 1), desc="Progress")
        
        # è®­ç»ƒå¾ªç¯
        for step in progress_bar:
            optimizer.zero_grad()
            total_adv_loss = 0.0
            total_bg_loss = 0.0
            total_similarity = 0.0

            # éå†æ‰€æœ‰èƒŒæ™¯å›¾è®¡ç®—æŸå¤±
            for img_tensor, pos, orig_patch in zip(
                img_tensors, positions, original_patches
            ):
                x, y = pos
                # å åŠ è¡¥ä¸ç”Ÿæˆå¯¹æŠ—å›¾åƒ
                adv_img = img_tensor.clone()
                clamp_patch = torch.clamp(patch, 0.0, 1.0)
                adv_img[:, :, y : y + patch_size, x : x + patch_size] = clamp_patch
                # print(f"patch èŒƒå›´:{patch.min().item():.6f}, {patch.max().item():.6f}")

                # è®¡ç®—å¯¹æŠ—æŸå¤±ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±ï¼‰
                img_emb = self.model.get_image_features(pixel_values=adv_img)
                img_emb = F.normalize(img_emb, dim=-1)
                similarity = F.cosine_similarity(img_emb, target_emb).mean()
                total_similarity += similarity
                adv_loss = 1 - similarity
                total_adv_loss += adv_loss

                # è®¡ç®—èƒŒæ™¯æŸå¤±ï¼ˆè¡¥ä¸ä¸åŸå§‹èƒŒæ™¯çš„MSEï¼‰
                bg_loss = F.mse_loss(patch, orig_patch)
                total_bg_loss += bg_loss

            # è®¡ç®—å¹³å‡æŸå¤±ä¸æ€»æŸå¤±
            num_imgs = len(img_tensors)
            avg_adv_loss = total_adv_loss / num_imgs
            avg_bg_loss = total_bg_loss / num_imgs
            total_loss = avg_adv_loss + background_weight * avg_bg_loss
            loss_history.append(total_loss.item())

            # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦å¹¶è®°å½•
            avg_similarity = total_similarity / num_imgs
            similarity_history.append(avg_similarity.item())

            # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            total_loss.backward()
            optimizer.step()
            patch.data = torch.clamp(patch.data, 0.0, 1.0)

            # æ—¶é—´è®¡ç®—ä¸æ ¼å¼åŒ–
            elapsed = datetime.now() - start_time
            progress = step / self.num_steps
            est_total = elapsed / progress if progress > 0 else elapsed
            remaining = est_total - elapsed
            elapsed_str = self._format_timedelta(elapsed)
            remaining_str = self._format_timedelta(remaining)

            # æ›´æ–°tqdmè¿›åº¦æ¡çš„æè¿°ä¿¡æ¯
            progress_bar.set_postfix({
                'å¯¹æŠ—æŸå¤±': f'{avg_adv_loss.item():.4f}',
                'èƒŒæ™¯æŸå¤±': f'{avg_bg_loss.item():.4f}',
                'æ€»æŸå¤±': f'{total_loss.item():.4f}',
                'ç›¸ä¼¼åº¦': f'{avg_similarity.item():.4f}',
                # 'å·²ç”¨æ—¶é—´': elapsed_str,
                # 'å‰©ä½™æ—¶é—´': remaining_str
            })

            # æ¯10æ­¥ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿å’Œç›¸ä¼¼åº¦æ›²çº¿
            if step % 10 == 0:
                self.plot_curves(loss_history, similarity_history)

            # æ‰¹é‡ä¿å­˜å½“å‰è¡¥ä¸ä¸æ‰€æœ‰å¯¹æŠ—èƒŒæ™¯å›¾ï¼ˆæ¯100æ­¥ï¼‰
            if step % 100== 0:
                self.save_adversarial_image(
                    img_tensors=img_tensors,
                    base_names=save_names,
                    patch=patch.detach(),
                    patch_size=patch_size,
                    positions=positions,
                )

        # è®­ç»ƒç»“æŸï¼šæœ€ç»ˆä¿å­˜æ‰€æœ‰ç»“æœ
        final_saved_paths = self.save_adversarial_image(
            img_tensors=img_tensors,
            base_names=save_names,
            patch=patch.detach(),
            patch_size=patch_size,
            positions=positions,
        )
        print(f"\nğŸ‰ è¡¥ä¸è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ–‡ä»¶ä¿å­˜è·¯å¾„: {final_saved_paths}")
        return final_saved_paths

    def plot_curves(self, loss_history, similarity_history):
        """è¾…åŠ©æ–¹æ³•ï¼šç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿å’Œç›¸ä¼¼åº¦æ›²çº¿"""
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        plt.figure(figsize=(8, 4))
        plt.plot(loss_history, label="Total Loss", color="#1f77b4")
        plt.xlabel("Training Step", fontsize=10)
        plt.ylabel("Loss Value", fontsize=10)
        plt.title(f"Patch Training Loss Curve", fontsize=12)
        plt.legend(fontsize=9)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        loss_path = os.path.join(
            self.output_dir, f"loss_curve_{self.timestamp}.png"
        )
        plt.savefig(loss_path, dpi=150)
        plt.close()

        
        # ç»˜åˆ¶ç›¸ä¼¼åº¦æ›²çº¿
        plt.figure(figsize=(8, 4))
        plt.plot(similarity_history, label="Cosine Similarity", color="#ff7f0e")
        plt.xlabel("Training Step", fontsize=10)
        plt.ylabel("Similarity Value", fontsize=10)
        plt.title(f"Embedding Similarity Curve", fontsize=12)
        plt.ylim(0, 1)  # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´åœ¨0-1ä¹‹é—´
        plt.legend(fontsize=9)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        similarity_path = os.path.join(
            self.output_dir, f"similarity_{self.timestamp}.png"
        )
        plt.savefig(similarity_path, dpi=150)
        plt.close()
