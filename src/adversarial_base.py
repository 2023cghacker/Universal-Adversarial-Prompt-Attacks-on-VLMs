import torch
import torch.nn.functional as F
from PIL import Image
from transformers import CLIPModel, CLIPProcessor
import os
import sys
from torchvision import transforms
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import sys


class CLIPAdversarialBase:
    def __init__(self, model_path, device=None, num_steps=300, lr=1e-3):
        """
        å¯¹æŠ—è®­ç»ƒåŸºç±»ï¼šå°è£…å…¬å…±åŠŸèƒ½ï¼ˆæ¨¡å‹åŠ è½½ã€é¢„å¤„ç†ã€ä¿å­˜ç­‰ï¼‰
        :param model_path: CLIPæ¨¡å‹æœ¬åœ°è·¯å¾„
        :param device: è®¡ç®—è®¾å¤‡ï¼ˆè‡ªåŠ¨é€‰æ‹©cuda/cpuï¼‰
        :param num_steps: è®­ç»ƒæ­¥æ•°
        :param lr: å­¦ä¹ ç‡
        """
        # è®¾å¤‡é…ç½®
        self.device = (
            device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        )
        self.num_steps = num_steps
        self.lr = lr

        # è¾“å‡ºç›®å½•ä¸æ—¶é—´æˆ³ï¼ˆæ‰€æœ‰å­ç±»å…±äº«ï¼‰
        self.timestamp = datetime.now().strftime("%m%d_%H%M%S")
        self.output_dir = os.path.join("output", self.timestamp)
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"è¾“å‡ºæ–‡ä»¶ä¿å­˜è·¯å¾„: {self.output_dir}")

        # åŠ è½½CLIPæ¨¡å‹ä¸å¤„ç†å™¨
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        self.model = CLIPModel.from_pretrained(model_path).to(self.device).eval()
        self.processor = CLIPProcessor.from_pretrained(model_path)

        # CLIPé»˜è®¤å›¾åƒæ ‡å‡†åŒ–å‚æ•°ï¼ˆImageNetï¼‰
        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)

    def load_and_preprocess_image(self, image_path):
        """å…¬å…±æ–¹æ³•ï¼šåŠ è½½å¹¶é¢„å¤„ç†å•å¼ å›¾åƒï¼ˆè¿”å›å¼ é‡ä¸åŸå§‹å›¾åƒï¼‰"""
        raw_img = Image.open(image_path).convert("RGB")
        image_inputs = self.processor(images=raw_img, return_tensors="pt")[
            "pixel_values"
        ].to(self.device)
        return image_inputs.clone().detach(), raw_img

    def get_target_text_embedding(self, target_text):
        """å…¬å…±æ–¹æ³•ï¼šè·å–ç›®æ ‡æ–‡æœ¬çš„å½’ä¸€åŒ–åµŒå…¥"""
        with torch.no_grad():
            text_inputs = self.processor(
                text=[target_text], return_tensors="pt", padding=True
            ).to(self.device)
            target_emb = self.model.get_text_features(**text_inputs)
            return F.normalize(target_emb, dim=-1)

    def get_target_image_embedding(self, target_img_path):
        """å…¬å…±æ–¹æ³•ï¼šè·å–ç›®æ ‡å›¾åƒçš„å½’ä¸€åŒ–åµŒå…¥"""
        with torch.no_grad():
            img_tensor, _ = self.load_and_preprocess_image(target_img_path)
            target_emb = self.model.get_image_features(pixel_values=img_tensor)
            return F.normalize(target_emb, dim=-1)

    def _denormalize(self, tensor):
        """å†…éƒ¨å…¬å…±æ–¹æ³•ï¼šå°†æ ‡å‡†åŒ–å¼ é‡åè½¬ä¸º0-255å›¾åƒæ ¼å¼"""
        mean = self.mean.to(tensor.device)
        std = self.std.to(tensor.device)
        tensor = tensor * std + mean
        return torch.clamp(tensor * 255.0, 0, 255).byte()

    def save_adversarial_image(
        self,
        img_tensors,
        base_names,
        patch=None,
        patch_size=80,
        positions=None,
        save_suffix=".png",
    ):
        """å…¬å…±æ–¹æ³•ï¼šæ‰¹é‡ä¿å­˜å¯¹æŠ—å›¾åƒï¼ˆæ”¯æŒå•patch+å¤šèƒŒæ™¯å›¾ï¼‰"""
        # è¾“å…¥åˆæ³•æ€§æ ¡éªŒ
        if len(img_tensors) != len(base_names):
            raise ValueError("å›¾åƒå¼ é‡åˆ—è¡¨ä¸åŸºç¡€åç§°åˆ—è¡¨é•¿åº¦å¿…é¡»ä¸€è‡´")
        if positions is not None and len(img_tensors) != len(positions):
            raise ValueError("å›¾åƒå¼ é‡åˆ—è¡¨ä¸ä½ç½®åˆ—è¡¨é•¿åº¦å¿…é¡»ä¸€è‡´")

        saved_paths = []

        # ä¿å­˜è¡¥ä¸ï¼ˆä»…ä¿å­˜1æ¬¡ï¼‰
        if patch is not None:
            patch_denorm = self._denormalize(patch.clone())
            patch_np = (
                patch_denorm.squeeze().cpu().permute(1, 2, 0).numpy().astype(np.uint8)
            )
            patch_path = os.path.join(
                self.output_dir, f"patch_{self.timestamp}{save_suffix}"
            )
            Image.fromarray(patch_np).save(patch_path)

        # æ‰¹é‡ä¿å­˜èƒŒæ™¯å›¾åƒï¼ˆå«è¡¥ä¸åº”ç”¨ï¼‰
        for i, (img_tensor, base_name) in enumerate(zip(img_tensors, base_names)):
            final_img = img_tensor.clone()
            # åº”ç”¨è¡¥ä¸ï¼ˆè‹¥æä¾›ï¼‰
            if patch is not None and positions is not None:
                x, y = positions[i]
                final_img[:, :, y : y + patch_size, x : x + patch_size] = patch

            # è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä¿å­˜
            final_denorm = self._denormalize(final_img.clone())
            final_np = (
                final_denorm.squeeze().cpu().permute(1, 2, 0).numpy().astype(np.uint8)
            )
            save_path = os.path.join(
                self.output_dir, f"{base_name}_{self.timestamp}{save_suffix}"
            )
            Image.fromarray(final_np).save(save_path)
            saved_paths.append(save_path)

        print(f"âœ… å·²ä¿å­˜ {len(saved_paths)} å¼ å¯¹æŠ—å›¾åƒ")
        return saved_paths

    def test(self, img_path, target_text=None, target_img=None):
        """å…¬å…±æ–¹æ³•ï¼šæµ‹è¯•å›¾åƒä¸ç›®æ ‡ï¼ˆæ–‡æœ¬/å›¾åƒï¼‰çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        # è¾“å…¥åˆæ³•æ€§æ ¡éªŒ
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {img_path}")
        if not ((target_text is None) ^ (target_img is None)):
            raise ValueError("target_textä¸target_imgå¿…é¡»äºŒé€‰ä¸€")

        # è®¡ç®—å›¾åƒåµŒå…¥ä¸ç›®æ ‡åµŒå…¥
        img_tensor, _ = self.load_and_preprocess_image(img_path)
        if target_text is not None:
            target_emb = self.get_target_text_embedding(target_text)
            target_info = f"æ–‡æœ¬: '{target_text}'"
        else:
            target_emb = self.get_target_image_embedding(target_img)
            target_info = f"å›¾åƒ: '{target_img}'"

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        with torch.no_grad():
            img_emb = self.model.get_image_features(pixel_values=img_tensor)
            img_emb = F.normalize(img_emb, dim=-1)
            similarity = F.cosine_similarity(img_emb, target_emb).mean()

        print(f"ğŸ“Š ç›®æ ‡{target_info}ä¸å›¾åƒçš„ä½™å¼¦ç›¸ä¼¼åº¦: {similarity.item():.6f}")
        return similarity.item()

    @staticmethod
    def _format_timedelta(td):
        """å†…éƒ¨é™æ€æ–¹æ³•ï¼šæ ¼å¼åŒ–æ—¶é—´ï¼ˆXh Ym Zsï¼‰"""
        total_sec = int(td.total_seconds())
        hours, rem = divmod(total_sec, 3600)
        minutes, seconds = divmod(rem, 60)
        return f"{hours}h {minutes}m {seconds}s"
