import torch
import torch.nn.functional as F
from PIL import Image
from transformers import CLIPModel, CLIPProcessor
import os
import sys
import shutil
from torchvision import transforms
import numpy as np
from datetime import datetime  # ç”¨äºæ—¶é—´æˆ³è®°å½•
from tqdm import tqdm


class AdversarialTrainer:
    def __init__(self, model_path, device=None, num_steps=300, lr=1e-3):
        """
        åˆå§‹åŒ–å¯¹æŠ—æ€§è®­ç»ƒå™¨
        :param model_path: CLIPæ¨¡å‹æœ¬åœ°è·¯å¾„
        :param device: è®¡ç®—è®¾å¤‡ï¼Œé»˜è®¤è‡ªåŠ¨é€‰æ‹©cuda/cpu
        :param num_steps: é»˜è®¤è®­ç»ƒæ­¥æ•°
        :param lr: é»˜è®¤å­¦ä¹ ç‡
        """
        # ç¡®å®šè®¡ç®—è®¾å¤‡
        self.device = (
            device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        )
        self.num_steps = num_steps
        self.lr = lr

        # è®°å½•åˆå§‹åŒ–æ—¶é—´æˆ³ï¼Œæ‰€æœ‰ä¿å­˜æ–‡ä»¶éƒ½ä½¿ç”¨è¿™ä¸ªæ—¶é—´æˆ³
        self.timestamp = datetime.now().strftime("%m%d_%H%M%S")  # å¢åŠ ç§’çº§ç²¾åº¦
        self.output_dir = os.path.join("output", self.timestamp)  # æŒ‰æ—¶é—´æˆ³åˆ›å»ºè¾“å‡ºç›®å½•

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜åˆ°: {self.output_dir}")

        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        self.model = CLIPModel.from_pretrained(model_path).to(self.device).eval()
        self.processor = CLIPProcessor.from_pretrained(model_path)

        # å›¾åƒæ ‡å‡†åŒ–å‚æ•°ï¼ˆCLIPé»˜è®¤ä½¿ç”¨ImageNetçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)

    def load_and_preprocess_image(self, image_path):
        """åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ"""
        raw_img = Image.open(image_path).convert("RGB")
        image_inputs = self.processor(images=raw_img, return_tensors="pt")[
            "pixel_values"
        ].to(self.device)
        return image_inputs.clone().detach(), raw_img

    def get_target_text_embedding(self, target_text):
        """è·å–ç›®æ ‡æ–‡æœ¬çš„ç‰¹å¾åµŒå…¥"""
        with torch.no_grad():
            text_inputs = self.processor(
                text=[target_text], return_tensors="pt", padding=True
            ).to(self.device)
            target_embedding = self.model.get_text_features(**text_inputs)
            return F.normalize(target_embedding, dim=-1)

    def get_target_image_embedding(self, target_img_path):
        """è·å–ç›®æ ‡å›¾åƒçš„ç‰¹å¾åµŒå…¥"""
        with torch.no_grad():
            img_tensor, _ = self.load_and_preprocess_image(target_img_path)
            target_embedding = self.model.get_image_features(pixel_values=img_tensor)
            return F.normalize(target_embedding, dim=-1)

    def _denormalize(self, tensor):
        """åæ ‡å‡†åŒ–å¼ é‡ä¸º0-255å›¾åƒæ ¼å¼ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        mean = self.mean.to(tensor.device)
        std = self.std.to(tensor.device)
        tensor = tensor * std + mean
        return torch.clamp(tensor * 255.0, 0, 255).byte()

    def save_adversarial_image(
        self,
        img_tensors,  # æ”¹ä¸ºæ¥å—å¼ é‡åˆ—è¡¨
        base_names,  # åŸºç¡€åç§°åˆ—è¡¨ï¼Œå¦‚["adv1", "adv2"]
        patch=None,
        patch_size=80,
        positions=None,  # ä½ç½®åˆ—è¡¨ï¼Œä¸å›¾åƒåˆ—è¡¨å¯¹åº”
        save_suffix=".png",
    ):
        """
        æ‰¹é‡ä¿å­˜å¯¹æŠ—æ€§å›¾åƒï¼Œä¸€æ¬¡æ€§å¤„ç†å¤šå¼ å›¾åƒ
        patchåªä¿å­˜ä¸€æ¬¡ï¼Œç¯å¢ƒå›¾åƒæ ¹æ®åˆ—è¡¨ä¿å­˜å¤šå¼ 
        """
        # ç¡®ä¿è¾“å…¥åˆ—è¡¨é•¿åº¦åŒ¹é…
        if len(img_tensors) != len(base_names):
            raise ValueError("å›¾åƒå¼ é‡åˆ—è¡¨å’ŒåŸºç¡€åç§°åˆ—è¡¨é•¿åº¦å¿…é¡»ç›¸åŒ")

        if positions is not None and len(img_tensors) != len(positions):
            raise ValueError("å›¾åƒå¼ é‡åˆ—è¡¨å’Œä½ç½®åˆ—è¡¨é•¿åº¦å¿…é¡»ç›¸åŒ")

        saved_paths = []

        # å…ˆä¿å­˜patchï¼ˆåªä¿å­˜ä¸€æ¬¡ï¼‰
        if patch is not None:
            patch_denorm = self._denormalize(patch.clone())
            patch_np = (
                patch_denorm.squeeze().cpu().permute(1, 2, 0).numpy().astype(np.uint8)
            )
            patch_filename = f"patch_{self.timestamp}{save_suffix}"
            patch_path = os.path.join(self.output_dir, patch_filename)
            Image.fromarray(patch_np).save(patch_path)
            print(f"ğŸ“Œ å·²ä¿å­˜Patchå›¾åƒ: {patch_path}")

        # æ‰¹é‡ä¿å­˜ç¯å¢ƒå›¾åƒ
        for i, (img_tensor, base_name) in enumerate(zip(img_tensors, base_names)):
            final_img = img_tensor.clone()

            # åº”ç”¨patchï¼ˆå¦‚æœæä¾›ï¼‰
            if patch is not None and positions is not None:
                x, y = positions[i]
                final_img[:, :, y : y + patch_size, x : x + patch_size] = patch

            # å¤„ç†å›¾åƒä¿å­˜
            final_denorm = self._denormalize(final_img.clone())
            final_np = (
                final_denorm.squeeze().cpu().permute(1, 2, 0).numpy().astype(np.uint8)
            )

            # æ„å»ºæ–‡ä»¶åï¼šåŸºç¡€åç§° + æ—¶é—´æˆ³ + åç¼€
            filename = f"{base_name}_{self.timestamp}{save_suffix}"
            save_path = os.path.join(self.output_dir, filename)
            Image.fromarray(final_np).save(save_path)
            print(f"ğŸ“Œ å·²ä¿å­˜å›¾åƒ: {save_path}")
            saved_paths.append(save_path)

        return saved_paths

    def train_perturbation(
        self,
        background_image_path,
        target_text,
        save_names,  # æ”¹ä¸ºä¿å­˜åç§°
        epsilon=0.15,
        save_suffix=".png",
    ):
        """è®­ç»ƒå…¨å›¾å¯¹æŠ—æ‰°åŠ¨"""

        img_tensor, _ = self.load_and_preprocess_image(background_image_path)
        target_embedding = self.get_target_text_embedding(target_text)

        # åˆå§‹åŒ–æ‰°åŠ¨
        perturbation = torch.randn_like(img_tensor) * 0.01
        perturbation = perturbation.clamp(-epsilon, epsilon).to(self.device)
        perturbation.requires_grad_(True)

        optimizer = torch.optim.Adam([perturbation], lr=self.lr)

        for step in range(self.num_steps):
            optimizer.zero_grad()
            adv_img = img_tensor + perturbation
            image_embedding = self.model.get_image_features(pixel_values=adv_img)
            image_embedding = F.normalize(image_embedding, dim=-1)

            loss = 1 - F.cosine_similarity(image_embedding, target_embedding).mean()
            loss.backward()
            optimizer.step()

            # é™åˆ¶æ‰°åŠ¨å¹…åº¦
            perturbation.data = torch.clamp(perturbation.data, -epsilon, epsilon)

            # æ¯éš”100è½®ä¿å­˜ä¸€æ¬¡ï¼ˆè¦†ç›–å¼ï¼‰
            if step % 100 == 0:
                self.save_adversarial_image(
                    img_tensors=[adv_img.detach()],  # ä¼ å…¥åˆ—è¡¨
                    base_names=save_names,
                    save_suffix=save_suffix,
                )

            if step % 20 == 0 or step == self.num_steps - 1:
                print(f"[æ‰°åŠ¨è®­ç»ƒ Step {step}] Loss: {loss.item():.6f}")
                sys.stdout.flush()

        # æœ€ç»ˆä¿å­˜
        return self.save_adversarial_image(
            img_tensors=[adv_img.detach()],  # ä¼ å…¥åˆ—è¡¨
            base_names=save_names,
            save_suffix=save_suffix,
        )

    def train_patch(
        self,
        background_image_paths,  # èƒŒæ™¯å›¾åƒè·¯å¾„åˆ—è¡¨
        target_text=None,
        target_img=None,
        patch_size=80,
        positions=[[30, 30]],  # ä½ç½®åˆ—è¡¨
        background_weight=0.1,
        initial_patch_path=None,
        save_names=None,  # ä¿å­˜åç§°åˆ—è¡¨ï¼Œå¦‚["adv1", "adv2"]
    ):
        """è®­ç»ƒå±€éƒ¨å¯¹æŠ—æ€§è¡¥ä¸ï¼Œæ”¯æŒå¤šèƒŒæ™¯å›¾åƒå’Œå¤šä½ç½®"""
        # æ£€æŸ¥target_textå’Œtarget_imgäº’æ–¥
        if not ((target_text is None) ^ (target_img is None)):
            raise ValueError("target_textå’Œtarget_imgå¿…é¡»ä¸”åªèƒ½æœ‰ä¸€ä¸ªä¸ºéNone")

        # æ£€æŸ¥èƒŒæ™¯å›¾åƒè·¯å¾„å’Œä½ç½®åˆ—è¡¨é•¿åº¦æ˜¯å¦åŒ¹é…
        if len(background_image_paths) != len(positions):
            raise ValueError("èƒŒæ™¯å›¾åƒè·¯å¾„åˆ—è¡¨å’Œä½ç½®åˆ—è¡¨é•¿åº¦å¿…é¡»ç›¸åŒ")

        # æ£€æŸ¥ä¿å­˜åç§°æ˜¯å¦æä¾›ä¸”é•¿åº¦åŒ¹é…
        if save_names is None or len(save_names) != len(background_image_paths):
            raise ValueError("ä¿å­˜åç§°åˆ—è¡¨å¿…é¡»æä¾›ä¸”ä¸èƒŒæ™¯å›¾åƒæ•°é‡ç›¸åŒ")

        # åŠ è½½æ‰€æœ‰èƒŒæ™¯å›¾åƒ
        img_tensors = []
        for img_path in background_image_paths:
            img_tensor, _ = self.load_and_preprocess_image(img_path)
            img_tensors.append(img_tensor)

        # æ£€æŸ¥æ‰€æœ‰ä½ç½®çš„æœ‰æ•ˆæ€§
        for i, (img_tensor, pos) in enumerate(zip(img_tensors, positions)):
            x, y = pos
            B, C, H, W = img_tensor.shape
            if x < 0 or x + patch_size > W or y < 0 or y + patch_size > H:
                raise ValueError(f"ç¬¬{i}ä¸ªpatchä½ç½®è¶…å‡ºå›¾åƒè¾¹ç•Œ: {W}x{H}")

        # è·å–ç›®æ ‡åµŒå…¥ï¼ˆæ–‡æœ¬æˆ–å›¾åƒï¼‰
        if target_text is not None:
            target_embedding = self.get_target_text_embedding(target_text)
        else:  # target_img is not None
            target_embedding = self.get_target_image_embedding(target_img)

        # åˆå§‹åŒ–è¡¥ä¸ï¼ˆæ ¹æ®åˆå§‹å›¾åƒæˆ–èƒŒæ™¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾åƒçš„èƒŒæ™¯åˆå§‹åŒ–ï¼‰
        if initial_patch_path is not None:
            transform = transforms.Compose(
                [
                    transforms.Resize((patch_size, patch_size)),
                    transforms.ToTensor(),
                    transforms.Lambda(
                        lambda x: x.unsqueeze(0)
                    ),  # å¢åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œå˜ä¸º[1, 3, patch_size, patch_size]
                ]
            )
            patch_img = Image.open(initial_patch_path).convert("RGB")
            patch_tensor = transform(patch_img)
            patch = patch_tensor * 0.9 + torch.randn_like(patch_tensor) * 0.1
            patch = patch.to(self.device)  # è½¬ç§»åˆ°è®¾å¤‡

        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾åƒçš„èƒŒæ™¯åˆå§‹åŒ–
            x, y = positions[0]
            background_patch = img_tensors[0][
                :, :, y : y + patch_size, x : x + patch_size
            ].clone()
            patch = background_patch * 0.8 + torch.randn_like(background_patch) * 0.2

        # ä¿å­˜åŸå§‹èƒŒæ™¯ï¼ˆç”¨äºå†…å®¹æŸå¤±è®¡ç®—ï¼‰
        original_patches = []
        for img_tensor, pos in zip(img_tensors, positions):
            x, y = pos
            original_patch = img_tensor[
                :, :, y : y + patch_size, x : x + patch_size
            ].detach()  # å‰¥ç¦»è®¡ç®—å›¾
            original_patches.append(original_patch)

        # å¦‚æœä½¿ç”¨åˆå§‹è¡¥ä¸ï¼Œä¹Ÿä¿å­˜åˆå§‹è¡¥ä¸
        if initial_patch_path is not None:
            initial_original_patch = patch_tensor.detach().to(self.device)
            original_patches.append(initial_original_patch)

        # å‡†å¤‡ä¼˜åŒ–
        patch.requires_grad_(True)
        optimizer = torch.optim.Adam([patch], lr=self.lr)

        for step in range(1, self.num_steps + 1):
            optimizer.zero_grad()

            total_adversarial_loss = 0.0
            total_background_loss = 0.0

            # å¯¹æ¯ä¸ªèƒŒæ™¯å›¾åƒå’Œä½ç½®è®¡ç®—æŸå¤±
            for img_tensor, pos, original_patch in zip(
                img_tensors, positions, original_patches[: len(img_tensors)]
            ):
                x, y = pos

                # åº”ç”¨è¡¥ä¸
                adv_img = img_tensor.clone()
                adv_img[:, :, y : y + patch_size, x : x + patch_size] = patch

                # è®¡ç®—æŸå¤±
                image_embedding = self.model.get_image_features(pixel_values=adv_img)
                image_embedding = F.normalize(image_embedding, dim=-1)

                # ç´¯åŠ å¯¹æŠ—æŸå¤±
                adversarial_loss = (
                    1 - F.cosine_similarity(image_embedding, target_embedding).mean()
                )
                total_adversarial_loss += adversarial_loss

                # ç´¯åŠ èƒŒæ™¯æŸå¤±
                background_loss = F.mse_loss(patch, original_patch)
                total_background_loss += background_loss

            # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆé™¤ä»¥å›¾åƒæ•°é‡ï¼‰
            num_images = len(img_tensors)
            avg_adversarial_loss = total_adversarial_loss / num_images
            avg_background_loss = total_background_loss / num_images
            total_loss = avg_adversarial_loss + background_weight * avg_background_loss

            # åå‘ä¼ æ’­
            total_loss.backward()
            optimizer.step()

            # æ¯éš”100è½®ä¿å­˜ä¸€æ¬¡å¹¶è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            if step % 100 == 0:
                # ä¸€æ¬¡æ€§ä¿å­˜æ‰€æœ‰å›¾åƒå’Œä¸€ä¸ªpatch
                self.save_adversarial_image(
                    img_tensors=img_tensors,
                    base_names=save_names,
                    patch=patch.detach(),
                    patch_size=patch_size,
                    positions=positions,
                    save_suffix=".png",
                )
                # è¾“å‡ºè¯¦ç»†ä¿¡æ¯
                print(
                    f"\n[è¡¥ä¸è®­ç»ƒ Step {step}] å¯¹æŠ—æŸå¤±: {avg_adversarial_loss.item():.6f}, "
                    f"èƒŒæ™¯æŸå¤±: {avg_background_loss.item():.6f}, "
                    f"æ€»æŸå¤±: {total_loss.item():.6f}"
                )
                sys.stdout.flush()

            # ç¬¬ä¸€æ­¥å’Œæœ€åä¸€æ­¥ä¹Ÿè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼ˆé™¤äº†å·²ç»è¢«100æ•´é™¤çš„æƒ…å†µï¼‰
            if (step == 1 or step == self.num_steps) and step % 100 != 0:
                print(
                    f"\n[è¡¥ä¸è®­ç»ƒ Step {step}] å¯¹æŠ—æŸå¤±: {avg_adversarial_loss.item():.6f}, "
                    f"èƒŒæ™¯æŸå¤±: {avg_background_loss.item():.6f}, "
                    f"æ€»æŸå¤±: {total_loss.item():.6f}"
                )
                sys.stdout.flush()

        # æœ€ç»ˆä¿å­˜æ‰€æœ‰å›¾åƒ
        return self.save_adversarial_image(
            img_tensors=img_tensors,
            base_names=save_names,
            patch=patch.detach(),
            patch_size=patch_size,
            positions=positions,
            save_suffix=".png",
        )

    def test(self, img_path, target_text=None, target_img=None):
        """
        è®¡ç®—å›¾åƒä¸ç›®æ ‡ï¼ˆæ–‡æœ¬æˆ–å›¾åƒï¼‰ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        """
        # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")

        # æ£€æŸ¥target_textå’Œtarget_imgäº’æ–¥
        if not ((target_text is None) ^ (target_img is None)):
            raise ValueError("target_textå’Œtarget_imgå¿…é¡»ä¸”åªèƒ½æœ‰ä¸€ä¸ªä¸ºéNone")

        # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
        img_tensor, _ = self.load_and_preprocess_image(img_path)

        # è·å–ç›®æ ‡åµŒå…¥ï¼ˆæ–‡æœ¬æˆ–å›¾åƒï¼‰
        if target_text is not None:
            target_embedding = self.get_target_text_embedding(target_text)
            target_info = f"æ–‡æœ¬: '{target_text}'"
        else:  # target_img is not None
            target_embedding = self.get_target_image_embedding(target_img)
            target_info = f"å›¾åƒ: '{target_img}'"

        # è®¡ç®—å›¾åƒåµŒå…¥
        with torch.no_grad():
            image_embedding = self.model.get_image_features(pixel_values=img_tensor)
            image_embedding = F.normalize(image_embedding, dim=-1)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = F.cosine_similarity(image_embedding, target_embedding).mean()
        print(f"ç›®æ ‡{target_info} ä¸å½“å‰å›¾åƒçš„ä½™å¼¦ç›¸ä¼¼åº¦: {similarity.item():.6f}")
        return similarity.item()


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    local_model_path = (
        "/HOME/paratera_xy/pxy480/HDD_POOL/Ling/downloads/clip-vit-large-patch14-336"
    )
    # èƒŒæ™¯å›¾åƒè·¯å¾„åˆ—è¡¨
    background_image_paths = [
        "images/pig.png",
        "images/another_background.png",
    ]
    # ä¿å­˜åç§°åˆ—è¡¨ï¼ˆä»…åç§°ï¼Œä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
    save_names = ["adv_pig", "adv_another"]
    # äºŒé€‰ä¸€ï¼šç›®æ ‡æ–‡æœ¬æˆ–ç›®æ ‡å›¾åƒ
    target_text = None  # "an apple"
    target_img = "images/apple.png"  # ç›®æ ‡å›¾åƒè·¯å¾„ï¼ˆä¸target_textäº’æ–¥ï¼‰
    train_mode = "patch"
    # ä½ç½®åˆ—è¡¨
    positions = [[30, 30], [50, 50]]

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = AdversarialTrainer(model_path=local_model_path, num_steps=300, lr=1e-3)
    print(f"ä½¿ç”¨è®¾å¤‡: {trainer.device}")
    print(f"ä¿å­˜æ–‡ä»¶æ—¶é—´æˆ³: {trainer.timestamp}")

    # è®­ç»ƒå¹¶ä¿å­˜ç»“æœ
    if train_mode == "perturbation":
        if target_text is None:
            raise ValueError("æ‰°åŠ¨è®­ç»ƒéœ€è¦target_textä¸ä¸ºNone")
        # æ‰°åŠ¨è®­ç»ƒåªä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾åƒ
        trainer.train_perturbation(
            background_image_path=background_image_paths[0],
            target_text=target_text,
            save_names=save_names[:1],  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªåç§°
        )
    elif train_mode == "patch":
        trainer.train_patch(
            background_image_paths=background_image_paths,
            target_text=target_text,
            target_img=target_img,
            patch_size=80,
            positions=positions,
            background_weight=0.2,
            save_names=save_names,
        )
    else:
        print("âŒ æ— æ•ˆçš„è®­ç»ƒæ¨¡å¼ï¼Œè¯·é€‰æ‹© 'perturbation' æˆ– 'patch'")
