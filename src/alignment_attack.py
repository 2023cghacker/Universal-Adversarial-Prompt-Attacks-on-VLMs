import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
from transformers import CLIPModel, CLIPProcessor
import os


def load_clip_model(model_path, device):
    """åŠ è½½æœ¬åœ°CLIPæ¨¡å‹å’Œå¤„ç†å™¨"""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")

    model = CLIPModel.from_pretrained(model_path).to(device).eval()
    processor = CLIPProcessor.from_pretrained(model_path)
    return model, processor


def load_and_preprocess_image(image_path, processor, device):
    """åŠ è½½å›¾åƒå¹¶è¿›è¡Œé¢„å¤„ç†"""
    raw_img = Image.open(image_path).convert("RGB")
    image_inputs = processor(images=raw_img, return_tensors="pt")["pixel_values"].to(device)
    return image_inputs.clone().detach(), raw_img


def initialize_perturbation(img_tensor, device, init_scale=0.01, epsilon=0.15):
    """åˆå§‹åŒ–å…¨å›¾å¯¹æŠ—æ‰°åŠ¨"""
    perturbation = (torch.randn_like(img_tensor) * init_scale).clamp(-epsilon, epsilon).to(device)
    perturbation.requires_grad_()
    return perturbation


def get_target_text_embedding(model, processor, target_text, device):
    """è·å–ç›®æ ‡æ–‡æœ¬çš„ç‰¹å¾åµŒå…¥"""
    with torch.no_grad():
        text_inputs = processor(text=[target_text], return_tensors="pt", padding=True).to(device)
        target_embedding = model.get_text_features(**text_inputs)
        return F.normalize(target_embedding, dim=-1)


def train_adversarial_perturbation(model, img_tensor, perturbation, target_embedding,
                                   device, num_steps=300, lr=1e-3, epsilon=0.15):
    """è®­ç»ƒå…¨å›¾å¯¹æŠ—æ‰°åŠ¨"""
    optimizer = torch.optim.Adam([perturbation], lr=lr)

    for step in range(num_steps):
        optimizer.zero_grad()
        adv_img = torch.clamp(img_tensor + perturbation, 0, 1)
        image_embedding = model.get_image_features(pixel_values=adv_img)
        image_embedding = F.normalize(image_embedding, dim=-1)
        loss = 1 - F.cosine_similarity(image_embedding, target_embedding).mean()
        loss.backward()
        optimizer.step()
        perturbation.data = torch.clamp(perturbation.data, -epsilon, epsilon)

        if step % 20 == 0 or step == num_steps - 1:
            print(f"[perturbation Step {step}] Loss: {loss.item():.6f}")

    return perturbation


def train_adversarial_patch(model, img_tensor, target_embedding, patch_size=80,
                            position='center', device='cuda', num_steps=300, lr=1e-2):
    """è®­ç»ƒå±€éƒ¨å¯¹æŠ—æ€§patchï¼ˆå·²ä¿®å¤non-leaf Tensoré”™è¯¯ï¼‰"""
    B, C, H, W = img_tensor.shape

    # ä¿®å¤ï¼šç¡®ä¿patchæ˜¯å¶å­å¼ é‡ï¼ˆå¯ä¼˜åŒ–ï¼‰
    patch = torch.randn((1, C, patch_size, patch_size), device=device)  # åˆå§‹éšæœºå¼ é‡ï¼ˆå¶å­å¼ é‡ï¼‰
    patch = patch * 0.01  # ç¼©æ”¾åˆå§‹åŒ–ï¼ˆä»ä¸ºå¶å­å¼ é‡ï¼‰
    patch.requires_grad_(True)  # æ‰‹åŠ¨å¼€å¯æ¢¯åº¦

    optimizer = torch.optim.Adam([patch], lr=lr)  # ç°åœ¨å¯æ­£å¸¸ä¼˜åŒ–

    def apply_patch(img, patch, position='center'):
        """å°†patchè´´åˆ°å›¾åƒæŒ‡å®šä½ç½®"""
        patched = img.clone()
        if position == 'center':
            x = (W - patch_size) // 2
            y = (H - patch_size) // 2
        elif position == 'top_left':
            x, y = 0, 0
        elif position == 'top_right':
            x, y = W - patch_size, 0
        elif position == 'bottom_left':
            x, y = 0, H - patch_size
        elif position == 'bottom_right':
            x, y = W - patch_size, H - patch_size
        else:
            raise ValueError("ä¸æ”¯æŒçš„patchä½ç½®ï¼Œå¯é€‰ï¼šcenter/top_left/top_right/bottom_left/bottom_right")

        patched[:, :, y:y + patch_size, x:x + patch_size] = patch
        return torch.clamp(patched, 0, 1)

    # è®­ç»ƒpatch
    for step in range(num_steps):
        optimizer.zero_grad()
        adv_img = apply_patch(img_tensor, patch, position)
        image_embedding = model.get_image_features(pixel_values=adv_img)
        image_embedding = F.normalize(image_embedding, dim=-1)
        loss = 1 - F.cosine_similarity(image_embedding, target_embedding).mean()
        loss.backward()
        optimizer.step()
        patch.data = torch.clamp(patch.data, 0, 1)  # ä¿æŒpatchåƒç´ åœ¨æœ‰æ•ˆèŒƒå›´

        if step % 20 == 0 or step == num_steps - 1:
            print(f"[patch Step {step}] Loss: {loss.item():.6f}")

    return patch.detach()


def save_adversarial_image(img_tensor, perturbation=None, patch=None, patch_size=80,
                           save_path="../output/adv_output.jpg", position='center'):
    """ä¿å­˜å¯¹æŠ—æ€§å›¾åƒï¼ˆæ”¯æŒå…¨å›¾æ‰°åŠ¨æˆ–å±€éƒ¨patchï¼‰"""
    final_img = img_tensor.clone()

    # åº”ç”¨å…¨å›¾æ‰°åŠ¨
    if perturbation is not None:
        final_img = torch.clamp(final_img + perturbation, 0, 1)

    # åº”ç”¨å±€éƒ¨patch
    if patch is not None:
        B, C, H, W = final_img.shape
        if position == 'center':
            x = (W - patch_size) // 2
            y = (H - patch_size) // 2
        elif position == 'top_left':
            x, y = 0, 0
        elif position == 'top_right':
            x, y = W - patch_size, 0
        elif position == 'bottom_left':
            x, y = 0, H - patch_size
        elif position == 'bottom_right':
            x, y = W - patch_size, H - patch_size
        else:
            raise ValueError("ä¸æ”¯æŒçš„patchä½ç½®")

        final_img[:, :, y:y + patch_size, x:x + patch_size] = patch
        final_img = torch.clamp(final_img, 0, 1)

    # ä¿å­˜å›¾åƒ
    final_img = final_img.squeeze().cpu()
    adv_pil = transforms.ToPILImage()(final_img)
    adv_pil.save(save_path)
    print(f"âœ… å¯¹æŠ—å›¾åƒä¿å­˜å®Œæ¯•: {save_path}")


def main():
    # å‚æ•°é…ç½®
    device = "cuda" if torch.cuda.is_available() else "cpu"
    local_model_path = r"/root/lc/clip-vit-large-patch14-336"  # æœ¬åœ°CLIPæ¨¡å‹è·¯å¾„
    image_path = "/root/lc/Universal-Adversarial-Prompt-Attacks-on-VLMs/images/pig.png"  # è¾“å…¥å›¾åƒè·¯å¾„
    target_text = "an apple"  # ç›®æ ‡æ–‡æœ¬ï¼ˆå¸Œæœ›å›¾åƒè¢«è¯¯åˆ†ç±»ä¸ºè¯¥æ–‡æœ¬ï¼‰

    # åŠ è½½æ¨¡å‹å’Œå›¾åƒ
    model, processor = load_clip_model(local_model_path, device)
    img_tensor, raw_img = load_and_preprocess_image(image_path, processor, device)
    target_embedding = get_target_text_embedding(model, processor, target_text, device)

    # é€‰æ‹©è®­ç»ƒæ¨¡å¼ï¼ˆäºŒé€‰ä¸€ï¼‰
    train_mode = "patch"  # å¯é€‰ï¼š"perturbation"ï¼ˆå…¨å›¾æ‰°åŠ¨ï¼‰æˆ– "patch"ï¼ˆå±€éƒ¨patchï¼‰
    print(f"ğŸ”§ è®­ç»ƒæ¨¡å¼: {train_mode}")
    if train_mode == "perturbation":
        # 1. è®­ç»ƒå…¨å›¾æ‰°åŠ¨
        perturbation = initialize_perturbation(img_tensor, device)
        perturbation = train_adversarial_perturbation(
            model, img_tensor, perturbation, target_embedding, device
        )
        save_adversarial_image(
            img_tensor, perturbation=perturbation, save_path="../output/adv_perturb.jpg"
        )
    elif train_mode == "patch":
        # 2. è®­ç»ƒå±€éƒ¨å¯¹æŠ—æ€§patch
        patch = train_adversarial_patch(
            model, img_tensor, target_embedding,
            patch_size=80,  # patchå°ºå¯¸
            position='center',  # patchä½ç½®ï¼ˆcenter/top_leftç­‰ï¼‰
            device=device,
            num_steps=300  # è®­ç»ƒæ­¥æ•°
        )
        save_adversarial_image(
            img_tensor, patch=patch, patch_size=80,
            save_path="../output/adv_patch.jpg", position='center'
        )
    else:
        print("âŒ æ— æ•ˆçš„è®­ç»ƒæ¨¡å¼ï¼Œè¯·é€‰æ‹© 'perturbation' æˆ– 'patch'")


if __name__ == "__main__":
    main()